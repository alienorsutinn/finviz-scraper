import math
from pathlib import Path
import sys

import numpy as np
import pandas as pd

# Ensure repo root is on sys.path so `import src...` works when running:
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# --- Config (data-driven guardrails) ---
MIN_N_INDUSTRY = 15
MIN_N_SECTOR = 30
MIN_EPS = 0.20  # minimum EPS for PE-based WFV (avoid tiny EPS artifacts)
WINSOR_P_LO = 0.01
WINSOR_P_HI = 0.99

# Scenario weights (match your default config)
MULTIPLIERS = {"bear": 0.7, "risk": 0.85, "base": 1.0, "bull": 1.2}
PROBS = {"bear": 0.25, "risk": 0.25, "base": 0.4, "bull": 0.1}

BANDS = {
    "aggressive_band": 0.8,   # STRONG_BUY
    "add_band_hi": 0.93,      # BUY
    "starter_band_hi": 1.0,   # TRIM
    "watch_band_hi": 1.15,    # WATCH
}

EXCLUDE_INDUSTRY_SUBSTR = (
    "Closed-End Fund",
    "ETF",
    "ETN",
)

def zone_from_ratio(r: float) -> str:
    if not math.isfinite(r) or r <= 0:
        return "WATCH"
    if r <= BANDS["aggressive_band"]:
        return "STRONG_BUY"
    if r <= BANDS["add_band_hi"]:
        return "BUY"
    if r <= BANDS["starter_band_hi"]:
        return "TRIM"
    if r <= BANDS["watch_band_hi"]:
        return "WATCH"
    return "AVOID"

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str:
    for c in candidates:
        if c in df.columns:
            return c
    raise KeyError(f"None of these columns found: {candidates}")

def main():
    inp = Path("data/latest/finviz_scored.parquet")
    outp = Path("data/latest/finviz_scored_peerwfv.parquet")

    df = pd.read_parquet(inp)

    # Normalize column names (strip + standardize sector/industry)
    df.columns = [str(c).strip() for c in df.columns]
    rename_map = {}

    # Standardize Sector/Industry -> sector/industry (case-insensitive)
    if "sector" not in df.columns:
        for c in df.columns:
            if str(c).lower() == "sector":
                rename_map[c] = "sector"
                break

    if "industry" not in df.columns:
        for c in df.columns:
            if str(c).lower() == "industry":
                rename_map[c] = "industry"
                break

    if rename_map:
        df = df.rename(columns=rename_map)

    price_col = pick_col(df, ["price", "close", "last"])
    eps_col = pick_col(df, ["eps_(ttm)", "eps_ttm", "eps", "raw__eps_(ttm)"])
    sector_col = pick_col(df, ["sector"])
    industry_col = pick_col(df, ["industry"])

    # Exclude obvious non-operating / fund-like instruments by industry string (cheap but effective)
    ind = df[industry_col].astype(str)
    is_excluded = ind.str.contains("|".join(map(str, EXCLUDE_INDUSTRY_SUBSTR)), case=False, na=False)

    # Positive EPS only for EPSÃ—multiple valuation
    eps = pd.to_numeric(df[eps_col], errors="coerce")
    price = pd.to_numeric(df[price_col], errors="coerce")

    implied_pe = price / eps.replace(0, np.nan)
    implied_pe = implied_pe.where((eps > 0) & np.isfinite(implied_pe) & (implied_pe > 0))

    # Winsorize implied PE globally (data-driven, kills micro-eps explosions)
    lo = implied_pe.quantile(WINSOR_P_LO)
    hi = implied_pe.quantile(WINSOR_P_HI)
    implied_pe_w = implied_pe.clip(lower=lo, upper=hi)

    # Group sizes on usable implied_pe
    grp_ind = [sector_col, industry_col]
    grp_sec = [sector_col]

    ind_n = implied_pe_w.groupby([df[c] for c in grp_ind]).transform("count")
    sec_n = implied_pe_w.groupby(df[sector_col]).transform("count")

    # Robust peer multiples: median (not mean)
    ind_med = implied_pe_w.groupby([df[c] for c in grp_ind]).transform("median")
    sec_med = implied_pe_w.groupby(df[sector_col]).transform("median")
    glob_med = float(implied_pe_w.median())

    peer_pe = ind_med.where(ind_n >= MIN_N_INDUSTRY)
    peer_level = pd.Series(np.where(ind_n >= MIN_N_INDUSTRY, "industry", None), index=df.index)

    peer_pe = peer_pe.fillna(sec_med.where(sec_n >= MIN_N_SECTOR))
    peer_level = peer_level.fillna(pd.Series(np.where(sec_n >= MIN_N_SECTOR, "sector", pd.NA), index=df.index))

    peer_pe = peer_pe.fillna(glob_med)
    peer_level = peer_level.fillna("global")

    # Build scenario fair values (anchors) and weighted fair value
    fair_base = eps * peer_pe
    fair_bear = fair_base * MULTIPLIERS["bear"]
    fair_risk = fair_base * MULTIPLIERS["risk"]
    fair_bull = fair_base * MULTIPLIERS["bull"]

    wfv = (
        PROBS["bear"] * fair_bear
        + PROBS["risk"] * fair_risk
        + PROBS["base"] * fair_base
        + PROBS["bull"] * fair_bull
    )

    # Apply exclusions & invalids
    wfv = wfv.where(~is_excluded)
    wfv = wfv.where(np.isfinite(wfv) & (wfv > 0))

    # --- Robust EPS for PE-based WFV ---
    # Cap extreme EPS within peer group (protect against bad data / one-offs)
    # IMPORTANT: only cap if the peer group has enough *positive EPS* names.
    # Otherwise (e.g., biotech), capping can incorrectly crush profitable outliers.
    MIN_POS_EPS_N = 10  # tune later
    MIN_POS_EPS_N = 10  # tune later
    # Robust group cols: prefer sector_col/industry_col, else existing grp/grp_cols, else common names
    cand = []
    for k in ('sector_col', 'industry_col'):
        v = locals().get(k)
        if isinstance(v, str) and v in df.columns:
            cand.append(v)

    # If prior grp/grp_cols exist but reference missing cols, filter them
    _prev = locals().get('grp_cols') or locals().get('grp')
    if _prev:
        if isinstance(_prev, (list, tuple)):
            prev_cols = [c for c in _prev if isinstance(c, str) and c in df.columns]
            if len(prev_cols) > len(cand):
                cand = prev_cols
        elif isinstance(_prev, str) and _prev in df.columns:
            cand = [_prev]

    # Fallback to common Finviz names
    if not cand:
        for c in ('sector', 'Sector', 'SECTOR'):
            if c in df.columns:
                cand.append(c)
                break
        for c in ('industry', 'Industry', 'INDUSTRY'):
            if c in df.columns:
                cand.append(c)
                break

    _grp_cols = tuple(dict.fromkeys(cand))  # dedupe, preserve order
    if not _grp_cols:
        raise KeyError(f"No sector/industry columns found. Available columns: {list(df.columns)}")

    _grp_cols = tuple([c for c in _grp_cols if c in df.columns])
    if not _grp_cols:
        raise KeyError(f"No sector/industry columns found. Available columns: {list(df.columns)}")
    pos_n = df.groupby(list(_grp_cols))[eps_col].transform(lambda s: (s > 0).sum())

    # [removed group EPS cap] eps_cap = df.groupby(list(_grp_cols))[eps_col].transform(
        # [removed group EPS cap] lambda s: (s[s > 0].quantile(0.90) if (s > 0).sum() >= MIN_POS_EPS_N else float('inf'))
    # [removed group EPS cap] )
    # eps_cap removed (no group EPS cap). Keep a debug column but set to +inf
    eps_cap = pd.Series(float('inf'), index=df.index)
    df['eps_cap_peer'] = eps_cap
    # --- EPS guardrails for peer WFV ---
    PE_FLOOR_FOR_EPS = 4.0
    MIN_EPS_FOR_WFV = 0.20
    price_col = 'price'
    
    # Start with EPS; drop tiny EPS that makes ratios meaningless
    df['eps_for_wfv'] = df[eps_col]
    df.loc[df['eps_for_wfv'] < MIN_EPS_FOR_WFV, 'eps_for_wfv'] = float('nan')
    
    PE_FLOOR_FOR_EPS = 8.0  # forced override (prevents 8x+ 'floor-driven' upside)
    
    # Cap EPS only when implied P/E is below the floor (EPS is implausibly large vs price)
    pe_calc = (df[price_col] / df['eps_for_wfv']).replace([float('inf'), -float('inf')], float('nan'))
    needs_cap = pe_calc.notna() & (pe_calc < PE_FLOOR_FOR_EPS) & df[price_col].notna()
    df.loc[needs_cap, 'eps_for_wfv'] = df.loc[needs_cap, price_col] / PE_FLOOR_FOR_EPS
    # Global safety cap for EPS (prevents single bad parse dominating upside)
    _gcap = df.loc[df[eps_col] > 0, eps_col].quantile(0.995)
    df['eps_for_wfv'] = df['eps_for_wfv'].clip(upper=_gcap)

    # --- PE-ratio WFV hardening (scale-safe) ---
    # Robust group cols (ensure they exist and are valid columns)
    if "_grp_cols" not in locals():
        cand = []
        for _nm in ("sector_col","industry_col"):
            if _nm in locals() and isinstance(locals()[_nm], str):
                cand.append(locals()[_nm])
        cand += ["sector","industry"]
        _grp_cols = tuple(dict.fromkeys([c for c in cand if c in df.columns]))
        if not _grp_cols:
            _grp_cols = ("sector",) if "sector" in df.columns else ("ticker",)

    # EPS sanity (avoid tiny/meaningless EPS making PE nonsense)
    MIN_EPS = 0.20
    ok_eps = df[eps_col].notna() & (df[eps_col] >= MIN_EPS)

    # PE guardrails: ignore absurd PEs (one-offs / bad data)
    MIN_PE = 3.0
    MAX_PE = 60.0

    # Compute pe_calc if missing; force NaN where EPS is not OK
    if "pe_calc" not in df.columns:
        df["pe_calc"] = (price / df[eps_col].replace(0, float("nan"))).where(ok_eps)
    else:
        df["pe_calc"] = df["pe_calc"].where(ok_eps)

    ok_pe = df["pe_calc"].notna() & (df["pe_calc"] >= MIN_PE) & (df["pe_calc"] <= MAX_PE)

    # Winsorize PE by peer group (only positive PEs). Fallback to MIN/MAX if group too thin.
    MIN_POS_PE_N = 10
    # Group key aligned to peer_level (sector vs industry)
    grp_key = np.where(
        df.get('peer_level', '').astype(str).str.lower().eq('industry'),
        'industry:' + df['industry'].astype(str),
        'sector:' + df['sector'].astype(str),
    )

    pe_lo = df.groupby(grp_key)["pe_calc"].transform(
        lambda s: s[s > 0].quantile(0.10) if (s > 0).sum() >= MIN_POS_PE_N else float("nan")
    )
    pe_hi = df.groupby(grp_key)["pe_calc"].transform(
        lambda s: s[s > 0].quantile(0.90) if (s > 0).sum() >= MIN_POS_PE_N else float("nan")
    )

    pe_lo = pe_lo.fillna(MIN_PE).clip(lower=MIN_PE)
    pe_hi = pe_hi.fillna(MAX_PE).clip(upper=MAX_PE)

    df["pe_for_wfv"] = df["pe_calc"].clip(lower=pe_lo, upper=pe_hi)

    # Scale-safe fair value from PE ratio (avoids EPS magnitude artifacts)
    wfv = price * (df["peer_pe_used"] / df["pe_for_wfv"].replace(0, float("nan")))

    # Final gating
    wfv = wfv.where(ok_eps & ok_pe)

    # Canonical peer WFV (compute first; gate later if needed)
    df['wfv_peer'] = df['eps_for_wfv'] * df['peer_pe_used']

    # EPS validity mask for PE-based valuation (avoid 0.01 EPS artifacts)
    MIN_EPS = 0.20  # tune later
    ok_eps = df[eps_col].notna() & (df[eps_col] >= MIN_EPS)
    MIN_POS_EPS_N = 10  # require enough positive-EPS peers for reliable caps
    ok_eps = ok_eps & (pos_n >= MIN_POS_EPS_N)

    # Eligibility for peer WFV (do NOT exclude floored/capped PEs; those are handled via eps_for_wfv/pe_for_wfv)
    ok_eps = (
        df['eps_for_wfv'].notna()
        & (df['eps_for_wfv'] > 0)
        & df['peer_pe_used'].notna()
        & (df['peer_pe_used'] > 0)
        & df['price'].notna()
        & (df['price'] > 0)
    )
    df.loc[~ok_eps, 'wfv_peer'] = float('nan')
    df["fair_bear_peer"] = fair_bear.where(~is_excluded)
    df["fair_risk_peer"] = fair_risk.where(~is_excluded)
    df["fair_base_peer"] = fair_base.where(~is_excluded)
    df["fair_bull_peer"] = fair_bull.where(~is_excluded)

    df["peer_pe_used"] = peer_pe.where(~is_excluded)
    # Guardrail: cap extreme peer multiples (keeps screen sane)
    df['peer_pe_used'] = df['peer_pe_used'].clip(lower=4.0, upper=40.0)
    df["peer_level"] = peer_level.where(~is_excluded)
    df["peer_n_industry"] = ind_n.where(~is_excluded)
    df["peer_n_sector"] = sec_n.where(~is_excluded)

    ratio = price / df["wfv_peer"].replace(0, np.nan)
    df["price_to_wfv_peer"] = ratio
    df["upside_pct_peer"] = (df["wfv_peer"] / price.replace(0, np.nan)) - 1.0
    df["zone_label_peer"] = ratio.map(lambda x: zone_from_ratio(float(x)) if pd.notna(x) else "WATCH")
    # If we couldn't compute WFV, mark explicitly
    df.loc[df['wfv_peer'].isna(), 'zone_label_peer'] = 'NO_WFV'

    outp.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(outp, index=False)

    print("wrote:", outp)
    print("wfv_peer non-null:", int(df["wfv_peer"].notna().sum()), "/", len(df))
    print("zone_label_peer counts:\n", df["zone_label_peer"].value_counts().head(20))

    # show worst extremes for sanity
    cols = [c for c in ["ticker", sector_col, industry_col, price_col, eps_col, "peer_pe_used", "peer_level", "peer_n_industry",
                        "wfv_peer", "price_to_wfv_peer", "upside_pct_peer", "zone_label_peer"] if c in df.columns]
    print("\nTop 20 upside (peer):\n", df.sort_values("upside_pct_peer", ascending=False)[cols].head(20).to_string(index=False))
    print("\nTop 20 overvalued (peer):\n", df.sort_values("price_to_wfv_peer", ascending=False)[cols].head(20).to_string(index=False))

if __name__ == "__main__":
    main()
